{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 6 - Tiled matmul\n",
    "\n",
    "> Let'start with a square matrix that is multiple of block width.\n",
    "\n",
    "TODO: Check what's the performance penalty of the boundary check. It might be better to just force matrices to be multiple of block size with padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda version: 12.8.0\n",
      "Device:\tNVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "cuda.init()\n",
    "\n",
    "device = cuda.Device(0)\n",
    "\n",
    "print(f\"Cuda version: {\".\".join([str(i) for i in cuda.get_version()])}\")\n",
    "print(f\"Device:\\t{device.name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[day_06_matmul-tiled.cu](https://github.com/xl0/cuda-100/blob/master/nbs/day_06_matmul-tiled.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "::: {.code-block}\n",
      "```\n",
      "#include <stdint.h>\n",
      "#include <stdio.h>\n",
      "\n",
      "\n",
      "// We will use square blocks to keep things sane.\n",
      "#define BLOCK_WIDTH 16\n",
      "\n",
      "\n",
      "__global__ void matmul_fp32_tiled(float* m1, float* m2, float* res,\n",
      "                                  uint32_t out_shape_0,\n",
      "                                  uint32_t out_shape_1,\n",
      "                                  uint32_t inner_dim,\n",
      "                                  uint32_t) {\n",
      "\n",
      "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "\n",
      "    __shared__ float m1_tile[TILE_WIDTH][TILE_WIDTH];\n",
      "    __shared__ float m2_tile[TILE_WIDTH][TILE_WIDTH];\n",
      "\n",
      "\n",
      "    int m1_x = inner_dim;\n",
      "    int m1_y = out_shape_0;\n",
      "\n",
      "    int m2_x = out_shape_1;\n",
      "    int m2_y = inner_dim;\n",
      "\n",
      "\n",
      "    // Assume the matrices are multiples my block size on both dims.\n",
      "\n",
      "    float R = 0;\n",
      "    for (int tile = 0; tile < inner_dim / TILE_WIDTH; tile++) {\n",
      "        m1_tile[threadIdx.y][threadIdx.x] = m1[y * m1_x + tile * TILE_WIDTH + threadIdx.x];\n",
      "        m2_tile[threadIdx.y][threadIdx.x] = m2[(tile * TILE_WIDTH + threadIdx.y) * m2_x + x];\n",
      "\n",
      "\n",
      "        __syncthreads();\n",
      "\n",
      "        for (int i = 0; i < TILE_WIDTH; i++) {\n",
      "            R += m1_tile[threadIdx.y][i] * m2_tile[i][threadIdx.x];\n",
      "        }\n",
      "\n",
      "        __syncthreads();\n",
      "    }\n",
      "\n",
      "    res[y * out_shape_1 + x] = R;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "// Non-tiled version\n",
      "__global__ void matmul_f32(float* m1, float* m2, float* res,\n",
      "                           uint32_t out_shape_0,\n",
      "                           uint32_t out_shape_1,\n",
      "                           uint32_t inner_dim,\n",
      "                           uint32_t) {\n",
      "\n",
      "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "\n",
      "    int m1_width = inner_dim;\n",
      "    int m2_width = out_shape_1;\n",
      "\n",
      "    float out;\n",
      "    if (x < out_shape_1 && y < out_shape_0) {\n",
      "        out = 0;\n",
      "        for (int i = 0; i < inner_dim; i++) {\n",
      "            out += m1[y * m1_width + i] * m2[i * m2_width + x];\n",
      "        }\n",
      "        res[y * out_shape_1 + x] = out;\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      ":::\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|output: asis\n",
    "#|echo: false\n",
    "\n",
    "c_code = Path('day_06_matmul-tiled.cu').read_text()\n",
    "print(f'''\n",
    "::: {{.code-block}}\n",
    "```\n",
    "{c_code}\n",
    "```\n",
    ":::\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_numpy import Lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array[1024, 1024] f32 n=1048576 (4Mb) xâˆˆ[-161.923, 148.814] Î¼=-0.029 Ïƒ=31.993"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = np.random.randn(1024, 1024).astype(np.float32)\n",
    "m2 = np.random.randn(1024, 1024).astype(np.float32)\n",
    "\n",
    "np_res = np.matmul(m1, m2)\n",
    "Lo(np_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompileError",
     "evalue": "nvcc compilation of /tmp/tmp__x_n_xt/kernel.cu failed\n[command: nvcc --cubin -Xcompiler -Wall -Xcompiler -Wextra -Xcompiler -Wsign-conversion -Xcompiler -Wcast-qual -Xcompiler -Wunused-parameter -Xcompiler -Wdouble-promotion -Xcompiler -Wformat=2 -Xcompiler -Wfloat-equal -Xcompiler -Wshadow -arch sm_86 -I/home/xl0/mambaforge/envs/cuda/lib/python3.12/site-packages/pycuda/cuda kernel.cu]\n[stderr:\nkernel.cu(19): error: identifier \"TILE_WIDTH\" is undefined\n      __attribute__((shared)) float m1_tile[TILE_WIDTH][TILE_WIDTH];\n                                            ^\n\nkernel.cu(24): warning #177-D: variable \"m1_y\" was declared but never referenced\n      int m1_y = out_shape_0;\n          ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nkernel.cu(27): warning #177-D: variable \"m2_y\" was declared but never referenced\n      int m2_y = inner_dim;\n          ^\n\n1 error detected in the compilation of \"kernel.cu\".\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mmake_context()\n\u001b[0;32m---> 12\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mSourceModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mday_06_matmul-tiled.cu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xcompiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xcompiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wextra\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xcompiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wsign-conversion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xcompiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wcast-qual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xcompiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wunused-parameter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xcompiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wdouble-promotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xcompiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wformat=2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xcompiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wfloat-equal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xcompiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wshadow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     matmul_tiled \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mget_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatmul_fp32_tiled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     gpu_m1 \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mmem_alloc_like(m1)\n",
      "File \u001b[0;32m~/mambaforge/envs/cuda/lib/python3.12/site-packages/pycuda/compiler.py:348\u001b[0m, in \u001b[0;36mSourceModule.__init__\u001b[0;34m(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    336\u001b[0m     source,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     include_dirs\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    345\u001b[0m ):\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_arch(arch)\n\u001b[0;32m--> 348\u001b[0m     cubin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnvcc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_extern_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43march\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_from_buffer\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;241m=\u001b[39m module_from_buffer(cubin)\n",
      "File \u001b[0;32m~/mambaforge/envs/cuda/lib/python3.12/site-packages/pycuda/compiler.py:297\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs, target)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m include_dirs:\n\u001b[1;32m    295\u001b[0m     options\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-I\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m i)\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_plain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnvcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/cuda/lib/python3.12/site-packages/pycuda/compiler.py:154\u001b[0m, in \u001b[0;36mcompile_plain\u001b[0;34m(source, options, keep, nvcc, cache_dir, target)\u001b[0m\n\u001b[1;32m    148\u001b[0m         warn(\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyCUDA: nvcc exited with status 0, but appears to have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencountered an error\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompileError\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompileError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvcc compilation of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m cu_file_path,\n\u001b[1;32m    156\u001b[0m         cmdline,\n\u001b[1;32m    157\u001b[0m         stdout\u001b[38;5;241m=\u001b[39mstdout\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    158\u001b[0m         stderr\u001b[38;5;241m=\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stdout \u001b[38;5;129;01mor\u001b[39;00m stderr:\n\u001b[1;32m    162\u001b[0m     lcase_err_text \u001b[38;5;241m=\u001b[39m (stdout \u001b[38;5;241m+\u001b[39m stderr)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[0;31mCompileError\u001b[0m: nvcc compilation of /tmp/tmp__x_n_xt/kernel.cu failed\n[command: nvcc --cubin -Xcompiler -Wall -Xcompiler -Wextra -Xcompiler -Wsign-conversion -Xcompiler -Wcast-qual -Xcompiler -Wunused-parameter -Xcompiler -Wdouble-promotion -Xcompiler -Wformat=2 -Xcompiler -Wfloat-equal -Xcompiler -Wshadow -arch sm_86 -I/home/xl0/mambaforge/envs/cuda/lib/python3.12/site-packages/pycuda/cuda kernel.cu]\n[stderr:\nkernel.cu(19): error: identifier \"TILE_WIDTH\" is undefined\n      __attribute__((shared)) float m1_tile[TILE_WIDTH][TILE_WIDTH];\n                                            ^\n\nkernel.cu(24): warning #177-D: variable \"m1_y\" was declared but never referenced\n      int m1_y = out_shape_0;\n          ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\nkernel.cu(27): warning #177-D: variable \"m2_y\" was declared but never referenced\n      int m2_y = inner_dim;\n          ^\n\n1 error detected in the compilation of \"kernel.cu\".\n]"
     ]
    }
   ],
   "source": [
    "BLOCK_SIZE = 16 # 16x16\n",
    "\n",
    "assert(len(m1.shape) == 2)\n",
    "assert(len(m2.shape) == 2)\n",
    "assert(m1.shape == m2.shape) # Make them equal for now\n",
    "\n",
    "out_shape = (m1.shape[0], m2.shape[1])\n",
    "\n",
    "try:\n",
    "    ctx = device.make_context()\n",
    "\n",
    "    mod = SourceModule(Path(\"day_06_matmul-tiled.cu\").read_text(),\n",
    "        options=[\n",
    "            '-Xcompiler', '-Wall',\n",
    "            '-Xcompiler', '-Wextra',\n",
    "            '-Xcompiler', '-Wsign-conversion',\n",
    "            '-Xcompiler', '-Wcast-qual',\n",
    "            '-Xcompiler', '-Wunused-parameter',\n",
    "            '-Xcompiler', '-Wdouble-promotion',\n",
    "            '-Xcompiler', '-Wformat=2',\n",
    "            '-Xcompiler', '-Wfloat-equal',\n",
    "            '-Xcompiler', '-Wshadow'\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    matmul_tiled = mod.get_function(\"matmul_fp32_tiled\")\n",
    "\n",
    "    gpu_m1 = cuda.mem_alloc_like(m1)\n",
    "    gpu_m2 = cuda.mem_alloc_like(m2)\n",
    "\n",
    "    res = np.empty(out_shape, dtype=np.float32)\n",
    "\n",
    "    gpu_res = cuda.mem_alloc_like(res)\n",
    "\n",
    "\n",
    "    cuda.memcpy_htod(gpu_m1, m1)\n",
    "    cuda.memcpy_htod(gpu_m2, m2)\n",
    "\n",
    "    block_size = (BLOCK_SIZE, BLOCK_SIZE, 1)\n",
    "    grid_size = (\n",
    "        ((out_shape[1] + BLOCK_SIZE - 1) // BLOCK_SIZE),\n",
    "        ((out_shape[0] + BLOCK_SIZE - 1) // BLOCK_SIZE),\n",
    "        1\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Matrix 1 shape: {m1.shape}\")\n",
    "    print(f\"Matrix 2 shape: {m2.shape}\")\n",
    "    print(f\"Result shape: {out_shape}\")\n",
    "    print(f\"Grid size: {grid_size}\")\n",
    "    print(f\"Block size: {block_size}\")\n",
    "    print(f\"Total threads: {grid_size[0] * grid_size[1] * block_size[0] * block_size[1]}\")\n",
    "\n",
    "    ctx.synchronize()\n",
    "\n",
    "    matmul_tiled(gpu_m1, gpu_m2, gpu_res, np.uint32(out_shape[1]), np.uint32(out_shape[0]), np.uint32(m1.shape[1]), grid=grid_size, block=block_size)\n",
    "\n",
    "    ctx.synchronize()\n",
    "\n",
    "    cuda.memcpy_dtoh(res, gpu_res)\n",
    "    ctx.synchronize()\n",
    "\n",
    "\n",
    "finally:\n",
    "    ctx.pop()\n",
    "    ctx.detach()\n",
    "\n",
    "Lo(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True, False,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]],\n",
       "      shape=(1024, 1024))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(res, np_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9794473648071289)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(res, np_res).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have the same numerical error situation as with naive matmul, let's compare with the non-tiled result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix 1 shape: (1024, 1024)\n",
      "Matrix 2 shape: (1024, 1024)\n",
      "Result shape: (1024, 1024)\n",
      "Grid size: (64, 64, 1)\n",
      "Block size: (16, 16, 1)\n",
      "Total threads: 1048576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array[1024, 1024] f32 n=1048576 (4Mb) xâˆˆ[-157.149, 144.693] Î¼=-0.019 Ïƒ=31.945"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLOCK_SIZE = 16 # 16x16\n",
    "\n",
    "assert(len(m1.shape) == 2)\n",
    "assert(len(m2.shape) == 2)\n",
    "assert(m1.shape == m2.shape) # Make them equal for now\n",
    "\n",
    "out_shape = (m1.shape[0], m2.shape[1])\n",
    "\n",
    "try:\n",
    "    ctx = device.make_context()\n",
    "\n",
    "    mod = SourceModule(Path(\"day_06_matmul-tiled.cu\").read_text(),\n",
    "        options=[\n",
    "            '-Xcompiler', '-Wall',\n",
    "            '-Xcompiler', '-Wextra',\n",
    "            '-Xcompiler', '-Wsign-conversion',\n",
    "            '-Xcompiler', '-Wcast-qual',\n",
    "            '-Xcompiler', '-Wunused-parameter',\n",
    "            '-Xcompiler', '-Wdouble-promotion',\n",
    "            '-Xcompiler', '-Wformat=2',\n",
    "            '-Xcompiler', '-Wfloat-equal',\n",
    "            '-Xcompiler', '-Wshadow'\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    matmul_naive = mod.get_function(\"matmul_f32\")\n",
    "\n",
    "    gpu_m1 = cuda.mem_alloc_like(m1)\n",
    "    gpu_m2 = cuda.mem_alloc_like(m2)\n",
    "\n",
    "    res_naive = np.empty(out_shape, dtype=np.float32)\n",
    "\n",
    "    gpu_res_naive = cuda.mem_alloc_like(res)\n",
    "\n",
    "\n",
    "    cuda.memcpy_htod(gpu_m1, m1)\n",
    "    cuda.memcpy_htod(gpu_m2, m2)\n",
    "\n",
    "    block_size = (BLOCK_SIZE, BLOCK_SIZE, 1)\n",
    "    grid_size = (\n",
    "        ((out_shape[1] + BLOCK_SIZE - 1) // BLOCK_SIZE),\n",
    "        ((out_shape[0] + BLOCK_SIZE - 1) // BLOCK_SIZE),\n",
    "        1\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Matrix 1 shape: {m1.shape}\")\n",
    "    print(f\"Matrix 2 shape: {m2.shape}\")\n",
    "    print(f\"Result shape: {out_shape}\")\n",
    "    print(f\"Grid size: {grid_size}\")\n",
    "    print(f\"Block size: {block_size}\")\n",
    "    print(f\"Total threads: {grid_size[0] * grid_size[1] * block_size[0] * block_size[1]}\")\n",
    "\n",
    "    ctx.synchronize()\n",
    "\n",
    "    matmul_naive(gpu_m1, gpu_m2, gpu_res_naive, np.uint32(out_shape[1]), np.uint32(out_shape[0]), np.uint32(m1.shape[1]), grid=grid_size, block=block_size)\n",
    "\n",
    "    ctx.synchronize()\n",
    "\n",
    "    cuda.memcpy_dtoh(res_naive, gpu_res_naive)\n",
    "    ctx.synchronize()\n",
    "\n",
    "\n",
    "finally:\n",
    "    ctx.pop()\n",
    "    ctx.detach()\n",
    "\n",
    "Lo(res_naive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res_naive == res).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yaaay, they match. This was the first attempt based on memory/understading of what I read in chapter 4, and it worked on the first try ðŸ˜Ž"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
