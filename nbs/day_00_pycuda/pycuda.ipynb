{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 0 - playing with PyCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "cuda.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's see what kind of GPUs we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda version: 12.8.0\n",
      "Device 0:\tNVIDIA GeForce RTX 3080 Laptop GPU\n",
      "\t\tCompute capability: 8.6\n",
      "\t\tVRAM used: 1504MiB / 16085MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MiB = 1024*1024\n",
    "\n",
    "print(f\"Cuda version: {\".\".join([str(i) for i in cuda.get_version()])}\")\n",
    "\n",
    "for i in range(cuda.Device.count()):\n",
    "    device = cuda.Device(i)\n",
    "\n",
    "    attrs = device.get_attributes()\n",
    "    context = device.make_context()\n",
    "\n",
    "    free_bytes, total_bytes = cuda.mem_get_info()\n",
    "    used_bytes = total_bytes - free_bytes\n",
    "\n",
    "    context.pop()\n",
    "    context.detach()\n",
    "\n",
    "    print(\n",
    "        f\"Device {i}:\\t{device.name()}\\n\"\n",
    "        f\"\\t\\tCompute capability: {\".\".join([str(i) for i in device.compute_capability()])}\\n\"\n",
    "        f\"\\t\\tVRAM used: {used_bytes // MiB}MiB / {total_bytes // MiB}MiB\\n\"\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb  7 00:25:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080 ...    Off |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   59C    P0             27W /  115W |    1344MiB /  16384MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi | head -n 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For some reason we get slightly less VRAM that's shown by nvidia-smi. I guess it's the memory reserved for CUDA stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's move some data in and out of the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "device = cuda.Device(0)\n",
    "\n",
    "try:\n",
    "    ctx = device.make_context()\n",
    "\n",
    "    cpu_array = np.random.randn(1024,1024).astype(np.float32)\n",
    "    gpu_array = cuda.mem_alloc_like(cpu_array)\n",
    "\n",
    "    cuda.memcpy_htod(gpu_array, cpu_array)\n",
    "\n",
    "    cpu_array_2 = np.empty_like(cpu_array, dtype=np.float32)\n",
    "\n",
    "    cuda.memcpy_dtoh(cpu_array_2, gpu_array)\n",
    "\n",
    "finally:\n",
    "    ctx.pop()\n",
    "    ctx.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cpu_array == cpu_array_2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looks ok. Let's try doing something with the data on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "ctx = device.make_context()\n",
    "\n",
    "try:\n",
    "  # Slightly expanded code from their tutorial.\n",
    "  mod = SourceModule(\"\"\"\n",
    "      __global__ void doublify(float *a)\n",
    "      {\n",
    "\n",
    "        int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "        int idx = y * blockDim.x * gridDim.x + x;\n",
    "\n",
    "        a[idx] *= 2;\n",
    "      }\n",
    "      \"\"\")\n",
    "\n",
    "  doublify = mod.get_function(\"doublify\")\n",
    "\n",
    "  # For a 1024x1024 array, we use a 32x32 grid of 32x32 blocks.\n",
    "  block_size = (32,32,1)\n",
    "  grid_size = (32,32,1)\n",
    "\n",
    "  cpu_array = np.random.randn(1024, 1024).astype(np.float32)\n",
    "  gpu_array = cuda.mem_alloc_like(cpu_array)\n",
    "  cpu_array_2 = np.empty_like(cpu_array, dtype=np.float32)\n",
    "\n",
    "  cuda.memcpy_htod(gpu_array, cpu_array)\n",
    "\n",
    "  doublify(gpu_array, block=block_size, grid=grid_size)\n",
    "\n",
    "  cuda.memcpy_dtoh(cpu_array_2, gpu_array)\n",
    "\n",
    "\n",
    "finally:\n",
    "  ctx.pop()\n",
    "  ctx.detach()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cpu_array_2 == (cpu_array * 2)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Looks like it worked! Tomorrow, I'll try it with C++."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
