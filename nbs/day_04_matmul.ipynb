{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 4 - Naive matmul+exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda version: 12.8.0\n",
      "Device:\tNVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "cuda.init()\n",
    "\n",
    "device = cuda.Device(0)\n",
    "\n",
    "print(f\"Cuda version: {\".\".join([str(i) for i in cuda.get_version()])}\")\n",
    "print(f\"Device:\\t{device.name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_file = \"kernels/matmul/matmul.cu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Naive matmul [kernels/matmul/matmul.cu](https://github.com/xl0/cuda-100/blob/master/nbs/kernels/matmul/matmul.cu)\n",
      "\n",
      "::: {.sourceCode}\n",
      "```cpp\n",
      "#include <stdint.h>\n",
      "#include <stdio.h>\n",
      "\n",
      "__global__ void matmul_f32(float *m1, float *m2, float *res,\n",
      "    uint32_t out_shape_0,\n",
      "    uint32_t out_shape_1,\n",
      "    uint32_t inner_dim,\n",
      "    uint32_t ) {\n",
      "\n",
      "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "\n",
      "    int m1_width = inner_dim;\n",
      "    int m2_width = out_shape_1;\n",
      "\n",
      "    double out;\n",
      "    if (x < out_shape_1 && y < out_shape_0) {\n",
      "        out = 0;\n",
      "        for (int i = 0; i < inner_dim; i++) {\n",
      "            out += m1[y*m1_width + i] * m2[i*m2_width + x];\n",
      "        }\n",
      "        res[y*out_shape_1 + x] = out;\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      ":::\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|output: asis\n",
    "#|echo: false\n",
    "\n",
    "c_code = Path(cu_file).read_text()\n",
    "print(f'''\n",
    "### Naive matmul [{cu_file}](https://github.com/xl0/cuda-100/blob/master/nbs/{cu_file})\n",
    "\n",
    "::: {{.sourceCode}}\n",
    "```cpp\n",
    "{c_code}\n",
    "```\n",
    ":::\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_numpy import Lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array[100, 300] f32 n=30000 (0.1Mb) x∈[-57.284, 65.644] μ=-0.102 σ=14.150"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = np.random.randn(100, 200).astype(np.float32)\n",
    "m2 = np.random.randn(200, 300).astype(np.float32)\n",
    "\n",
    "np_res = np.matmul(m1, m2)\n",
    "Lo(np_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing naive matnul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: (10, 4, 1)\n",
      "Block size: (32, 32, 1)\n",
      "Restul dimensions: 100x300\n",
      "Total threads: 40960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array[100, 300] f32 n=30000 (0.1Mb) x∈[-57.284, 65.644] μ=-0.102 σ=14.150"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLOCK_SIZE_X = 32\n",
    "BLOCK_SIZE_Y = 32\n",
    "\n",
    "assert(len(m1.shape) == 2)\n",
    "assert(len(m2.shape) == 2)\n",
    "assert(m1.shape[1] == m2.shape[0])\n",
    "\n",
    "out_shape = (m1.shape[0], m2.shape[1])\n",
    "\n",
    "try:\n",
    "    ctx = device.make_context()\n",
    "\n",
    "    mod = SourceModule(Path(cu_file).read_text(),\n",
    "        options=[\n",
    "            '-Xcompiler', '-Wall',\n",
    "            '-Xcompiler', '-Wextra',\n",
    "            '-Xcompiler', '-Wsign-conversion',\n",
    "            '-Xcompiler', '-Wcast-qual',\n",
    "            '-Xcompiler', '-Wunused-parameter',\n",
    "            '-Xcompiler', '-Wdouble-promotion',\n",
    "            '-Xcompiler', '-Wformat=2',\n",
    "            '-Xcompiler', '-Wfloat-equal',\n",
    "            '-Xcompiler', '-Wshadow'\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    matmul_f32 = mod.get_function(\"matmul_f32\")\n",
    "\n",
    "    gpu_m1 = cuda.mem_alloc_like(m1)\n",
    "    gpu_m2 = cuda.mem_alloc_like(m2)\n",
    "\n",
    "    res = np.empty(out_shape, dtype=np.float32)\n",
    "\n",
    "    gpu_res = cuda.mem_alloc_like(res)\n",
    "\n",
    "\n",
    "    cuda.memcpy_htod(gpu_m1, m1)\n",
    "    cuda.memcpy_htod(gpu_m2, m2)\n",
    "\n",
    "    block_size = (BLOCK_SIZE_X, BLOCK_SIZE_Y, 1)\n",
    "    grid_size = (\n",
    "        ((out_shape[1] + BLOCK_SIZE_X - 1) // BLOCK_SIZE_X),\n",
    "        ((out_shape[0] + BLOCK_SIZE_Y - 1) // BLOCK_SIZE_Y),\n",
    "        1\n",
    "    )\n",
    "\n",
    "    print(f\"Grid size: {grid_size}\")\n",
    "    print(f\"Block size: {block_size}\")\n",
    "    print(f\"Restul dimensions: {out_shape[0]}x{out_shape[1]}\")\n",
    "    print(f\"Total threads: {grid_size[0] * grid_size[1] * block_size[0] * block_size[1]}\")\n",
    "\n",
    "    ctx.synchronize()\n",
    "\n",
    "    matmul_f32(gpu_m1, gpu_m2, gpu_res, np.uint32(out_shape[0]), np.uint32(out_shape[1]), np.uint32(m1.shape[1]), grid=grid_size, block=block_size)\n",
    "\n",
    "    ctx.synchronize()\n",
    "\n",
    "    cuda.memcpy_dtoh(res, gpu_res)\n",
    "    ctx.synchronize()\n",
    "\n",
    "\n",
    "finally:\n",
    "    ctx.pop()\n",
    "    ctx.detach()\n",
    "\n",
    "Lo(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float32(-0.9388011), np.float32(0.16297509))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1[0,1],m2[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(res, np_res).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<lovely_numpy.repr_chans.ChanProxy at 0x713228fc82c0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lo(np.isclose(res, np_res)).chans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nunmeric stability\n",
    "Looks like matmul is very succeptible to numerical instability.\n",
    "> Since we are adding numbers to the accumulator over and over, if the accumulated value gets large enough,\n",
    "> it will lose precision to correctly accumulate small values. If it then gets a large update opposite of accumulated value\n",
    "> and becomes small again, those errors will become very significant.\n",
    "\n",
    "But I think overall it's correct. I changed to accumulator to be double, and still seeting the discrepancy. It's possible\n",
    "that numpy matmul also not not very precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Let's do the exercises\n",
    "\n",
    "1. In this chapter we implemented a matrix multiplication kernel that has each thread produce one output matrix element. In this question, you will implement different matrix-matrix multiplication kernels and compare them.\n",
    "    - a. Write a kernel that has each thread produce one output matrix row. Fill in the execution configuration parameters for the design.\n",
    "    - b. Write a kernel that has each thread produce one output matrix column. Fill in the execution configuration parameters for the design.\n",
    "    - c. Analyze the pros and cons of each of the two kernel designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_file2 = \"kernels/matmul/matmul-row_col.cu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thread per row/col:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Naive matmul row/col [kernels/matmul/matmul-row_col.cu](https://github.com/xl0/cuda-100/blob/master/nbs/kernels/matmul/matmul-row_col.cu)\n",
      "\n",
      "::: {.SourceCode}\n",
      "```cpp\n",
      "#include <stdint.h>\n",
      "#include <stdio.h>\n",
      "\n",
      "__global__ void matmul_f32(float* m1, float* m2, float* res,\n",
      "                           uint32_t out_shape_0,\n",
      "                           uint32_t out_shape_1,\n",
      "                           uint32_t inner_dim)\n",
      "{\n",
      "\n",
      "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "\n",
      "    int m1_width = inner_dim;\n",
      "    int m2_width = out_shape_1;\n",
      "\n",
      "    double out;\n",
      "    if (x < out_shape_1 && y < out_shape_0) {\n",
      "        out = 0;\n",
      "        for (int i = 0; i < inner_dim; i++) {\n",
      "            out += m1[y * m1_width + i] * m2[i * m2_width + x];\n",
      "        }\n",
      "        res[y * out_shape_1 + x] = out;\n",
      "    }\n",
      "}\n",
      "\n",
      "__global__ void matmul_f32_row(float* m1, float* m2, float* res,\n",
      "                               uint32_t out_shape_0,\n",
      "                               uint32_t out_shape_1,\n",
      "                               uint32_t inner_dim,\n",
      "                               uint32_t)\n",
      "\n",
      "{\n",
      "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "    int m1_width = inner_dim;\n",
      "    int m2_width = out_shape_1;\n",
      "\n",
      "    if (y < out_shape_0) {\n",
      "        for (int x = 0; x < out_shape_1; x++) {\n",
      "            double out = 0;\n",
      "            for (int i = 0; i < inner_dim; i++) {\n",
      "                out += m1[y * m1_width + i] * m2[i * m2_width + x];\n",
      "            }\n",
      "            res[y * out_shape_1 + x] = out;\n",
      "        }\n",
      "    }\n",
      "\n",
      "}\n",
      "\n",
      "__global__ void matmul_f32_col(float* m1, float* m2, float* res,\n",
      "                               uint32_t out_shape_0,\n",
      "                               uint32_t out_shape_1,\n",
      "                               uint32_t inner_dim,\n",
      "                               uint32_t)\n",
      "{\n",
      "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    int m1_width = inner_dim;\n",
      "    int m2_width = out_shape_1;\n",
      "\n",
      "    if (x < out_shape_1) {\n",
      "        for (int y = 0; y < out_shape_1; y++) {\n",
      "            double out = 0;\n",
      "            for (int i = 0; i < inner_dim; i++) {\n",
      "                out += m1[y * m1_width + i] * m2[i * m2_width + x];\n",
      "            }\n",
      "            res[y * out_shape_1 + x] = out;\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "```\n",
      ":::\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|output: asis\n",
    "#|echo: false\n",
    "\n",
    "c_code = Path(cu_file2).read_text()\n",
    "print(f'''\n",
    "### Naive matmul row/col [{cu_file2}](https://github.com/xl0/cuda-100/blob/master/nbs/{cu_file2})\n",
    "\n",
    "::: {{.SourceCode}}\n",
    "```cpp\n",
    "{c_code}\n",
    "```\n",
    ":::\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the thread per row matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: (1, 4, 1)\n",
      "Block size: (1, 32, 1)\n",
      "Restul dimensions: 100x300\n",
      "Total threads: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array[100, 300] f32 n=30000 (0.1Mb) x∈[-57.284, 65.644] μ=-0.102 σ=14.150"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLOCK_SIZE_X = 1\n",
    "BLOCK_SIZE_Y = 32\n",
    "\n",
    "assert(len(m1.shape) == 2)\n",
    "assert(len(m2.shape) == 2)\n",
    "assert(m1.shape[1] == m2.shape[0])\n",
    "\n",
    "out_shape = (m1.shape[0], m2.shape[1])\n",
    "\n",
    "try:\n",
    "    ctx = device.make_context()\n",
    "\n",
    "    mod = SourceModule(Path(cu_file2).read_text(),\n",
    "        options=[\n",
    "            '-Xcompiler', '-Wall',\n",
    "            '-Xcompiler', '-Wextra',\n",
    "            '-Xcompiler', '-Wsign-conversion',\n",
    "            '-Xcompiler', '-Wcast-qual',\n",
    "            '-Xcompiler', '-Wunused-parameter',\n",
    "            '-Xcompiler', '-Wdouble-promotion',\n",
    "            '-Xcompiler', '-Wformat=2',\n",
    "            '-Xcompiler', '-Wfloat-equal',\n",
    "            '-Xcompiler', '-Wshadow'\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    matmul_f32_row = mod.get_function(\"matmul_f32_row\")\n",
    "\n",
    "    gpu_m1 = cuda.mem_alloc_like(m1)\n",
    "    gpu_m2 = cuda.mem_alloc_like(m2)\n",
    "\n",
    "    res_row = np.empty(out_shape, dtype=np.float32)\n",
    "\n",
    "    gpu_res_row = cuda.mem_alloc_like(res_row)\n",
    "\n",
    "\n",
    "    cuda.memcpy_htod(gpu_m1, m1)\n",
    "    cuda.memcpy_htod(gpu_m2, m2)\n",
    "\n",
    "    block_size = (BLOCK_SIZE_X, BLOCK_SIZE_Y, 1)\n",
    "    grid_size = (1, ((out_shape[0] + BLOCK_SIZE_Y - 1) // BLOCK_SIZE_Y), 1)\n",
    "\n",
    "    print(f\"Grid size: {grid_size}\")\n",
    "    print(f\"Block size: {block_size}\")\n",
    "    print(f\"Restul dimensions: {out_shape[0]}x{out_shape[1]}\")\n",
    "    print(f\"Total threads: {grid_size[0] * grid_size[1] * block_size[0] * block_size[1]}\")\n",
    "\n",
    "    ctx.synchronize()\n",
    "\n",
    "    matmul_f32_row(gpu_m1, gpu_m2, gpu_res_row, np.uint32(out_shape[0]), np.uint32(out_shape[1]), np.uint32(m1.shape[1]), grid=grid_size, block=block_size)\n",
    "    # ctx.synchronize()\n",
    "\n",
    "    cuda.memcpy_dtoh(res_row, gpu_res_row)\n",
    "    ctx.synchronize()\n",
    "\n",
    "\n",
    "finally:\n",
    "    ctx.pop()\n",
    "    ctx.detach()\n",
    "\n",
    "Lo(res_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array[100, 300] f32 n=30000 (0.1Mb) x∈[-57.284, 65.644] μ=-0.102 σ=14.150"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lo(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res == res_row).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing one thread per col matnul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: (10, 1, 1)\n",
      "Block size: (32, 1, 1)\n",
      "Restul dimensions: 100x300\n",
      "Total threads: 320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array[100, 300] f32 n=30000 (0.1Mb) x∈[-57.284, 65.644] μ=-0.102 σ=14.150"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLOCK_SIZE_X = 32\n",
    "BLOCK_SIZE_Y = 1\n",
    "\n",
    "assert(len(m1.shape) == 2)\n",
    "assert(len(m2.shape) == 2)\n",
    "assert(m1.shape[1] == m2.shape[0])\n",
    "\n",
    "out_shape = (m1.shape[0], m2.shape[1])\n",
    "\n",
    "try:\n",
    "    ctx = device.make_context()\n",
    "\n",
    "    mod = SourceModule(Path(cu_file2).read_text(),\n",
    "        options=[\n",
    "            '-Xcompiler', '-Wall',\n",
    "            '-Xcompiler', '-Wextra',\n",
    "            '-Xcompiler', '-Wsign-conversion',\n",
    "            '-Xcompiler', '-Wcast-qual',\n",
    "            '-Xcompiler', '-Wunused-parameter',\n",
    "            '-Xcompiler', '-Wdouble-promotion',\n",
    "            '-Xcompiler', '-Wformat=2',\n",
    "            '-Xcompiler', '-Wfloat-equal',\n",
    "            '-Xcompiler', '-Wshadow'\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    matmul_f32_col = mod.get_function(\"matmul_f32_col\")\n",
    "\n",
    "    gpu_m1 = cuda.mem_alloc_like(m1)\n",
    "    gpu_m2 = cuda.mem_alloc_like(m2)\n",
    "\n",
    "    res_col = np.empty(out_shape, dtype=np.float32)\n",
    "\n",
    "    gpu_res_col = cuda.mem_alloc_like(res_col)\n",
    "\n",
    "\n",
    "    cuda.memcpy_htod(gpu_m1, m1)\n",
    "    cuda.memcpy_htod(gpu_m2, m2)\n",
    "\n",
    "    block_size = (BLOCK_SIZE_X, BLOCK_SIZE_Y, 1)\n",
    "    grid_size = (((out_shape[1] + BLOCK_SIZE_X - 1) // BLOCK_SIZE_X),1,1)\n",
    "\n",
    "    print(f\"Grid size: {grid_size}\")\n",
    "    print(f\"Block size: {block_size}\")\n",
    "    print(f\"Restul dimensions: {out_shape[0]}x{out_shape[1]}\")\n",
    "    print(f\"Total threads: {grid_size[0] * grid_size[1] * block_size[0] * block_size[1]}\")\n",
    "\n",
    "    ctx.synchronize()\n",
    "\n",
    "    matmul_f32_col(gpu_m1, gpu_m2, gpu_res_col, np.uint32(out_shape[0]), np.uint32(out_shape[1]), np.uint32(m1.shape[1]), grid=grid_size, block=block_size)\n",
    "    ctx.synchronize()\n",
    "\n",
    "    cuda.memcpy_dtoh(res_col, gpu_res_col)\n",
    "    ctx.synchronize()\n",
    "\n",
    "\n",
    "finally:\n",
    "    ctx.pop()\n",
    "    ctx.detach()\n",
    "\n",
    "Lo(res_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res_col == res).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  c. Analyze the pros and cons of each of the two kernel designs.\n",
    "```\n",
    "They both suck, but because we are not using enough threads to saturate the GPU.\n",
    "Row possibly sucks less because the cache is shared between thread blocks.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
