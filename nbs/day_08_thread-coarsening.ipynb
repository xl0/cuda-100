{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 7 - Tiled matmul experiments\n",
    "\n",
    "- Benchmark it against Numpy and naive matmul\n",
    "- Benchmark the impact of boundary checks on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda version: 12.8.0\n",
      "Device:\tNVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "cuda.init()\n",
    "\n",
    "device = cuda.Device(0)\n",
    "\n",
    "print(f\"Cuda version: {\".\".join([str(i) for i in cuda.get_version()])}\")\n",
    "print(f\"Device:\\t{device.name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_file=\"day_08_thread-coarsening.cu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[day_08_thread-coarsening.cu](https://github.com/xl0/cuda-100/blob/master/nbs/day_08_thread-coarsening.cu)\n",
      "\n",
      "::: {.code-block}\n",
      "```\n",
      "#include <stdint.h>\n",
      "#include <stdio.h>\n",
      "\n",
      "#ifndef TILE_WIDTH\n",
      "#ifdef __INTELLISENSE__\n",
      "#define TILE_WIDTH 16\n",
      "#else\n",
      "#error \"TILE_WIDTH must be defined\"\n",
      "#endif\n",
      "#endif\n",
      "\n",
      "#ifndef THREAD_COARSENING\n",
      "#ifdef __INTELLISENSE__\n",
      "#define THREAD_COARSENING 2\n",
      "#else\n",
      "#error \"THREAD_COARSENING must be defined\"\n",
      "#endif\n",
      "#endif\n",
      "\n",
      "\n",
      "__global__ void matmul_fp32_tiled(float *m1, float *m2, float *res, uint32_t out_shape_0,\n",
      "                                  uint32_t out_shape_1, uint32_t inner_dim, uint32_t) {\n",
      "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "\n",
      "    __shared__ float m1_tile[TILE_WIDTH][TILE_WIDTH];\n",
      "    __shared__ float m2_tile[TILE_WIDTH][TILE_WIDTH];\n",
      "\n",
      "    int m1_x = inner_dim;\n",
      "    int m2_x = out_shape_1;\n",
      "\n",
      "    // Assume the matrices are multiples my block size on both dims.\n",
      "\n",
      "    float R = 0;\n",
      "    for (int tile = 0; tile < inner_dim / TILE_WIDTH; tile++) {\n",
      "        m1_tile[threadIdx.y][threadIdx.x] = m1[y * m1_x + tile * TILE_WIDTH + threadIdx.x];\n",
      "        m2_tile[threadIdx.y][threadIdx.x] = m2[(tile * TILE_WIDTH + threadIdx.y) * m2_x + x];\n",
      "\n",
      "        __syncthreads();\n",
      "\n",
      "        for (int i = 0; i < TILE_WIDTH; i++) {\n",
      "            R += m1_tile[threadIdx.y][i] * m2_tile[i][threadIdx.x];\n",
      "        }\n",
      "\n",
      "        __syncthreads();\n",
      "    }\n",
      "\n",
      "    res[y * out_shape_1 + x] = R;\n",
      "}\n",
      "\n",
      "\n",
      "__global__ void matmul_fp32_tiled_coarse(float *m1, float *m2, float *res, uint32_t out_shape_0,\n",
      "                                         uint32_t out_shape_1, uint32_t inner_dim, uint32_t) {\n",
      "    int x = blockIdx.x * blockDim.x * THREAD_COARSENING + threadIdx.x;\n",
      "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "\n",
      "    // if (threadIdx.x == 0 && threadIdx.y == 0) {\n",
      "    //     printf(\"blockIdx = (%d, %d), mx = %d, y = %d\\n\", blockIdx.x, blockIdx.y, x, y);\n",
      "    // }\n",
      "\n",
      "    __shared__ float m1_tile[TILE_WIDTH][TILE_WIDTH];\n",
      "    __shared__ float m2_tile[TILE_WIDTH][TILE_WIDTH];\n",
      "\n",
      "    float R[THREAD_COARSENING];\n",
      "    for (int i = 0; i < THREAD_COARSENING; i++) {\n",
      "        R[i] = 0;\n",
      "    }\n",
      "\n",
      "    int m1_x = inner_dim;\n",
      "    int m2_x = out_shape_1;\n",
      "\n",
      "    // We are going to coarse the thread over x, so let's load the tile from the\n",
      "    // second matrix.\n",
      "\n",
      "    for (int tile = 0; tile < inner_dim / TILE_WIDTH; tile++) {\n",
      "        m1_tile[threadIdx.y][threadIdx.x] = m1[y * m1_x + tile * TILE_WIDTH + threadIdx.x];\n",
      "\n",
      "        // Now, we are going to calculate a bunch consecutive tiles one by one,\n",
      "        // so we need to load the\n",
      "        for (int c = 0; c < THREAD_COARSENING; c++) {\n",
      "            m2_tile[threadIdx.y][threadIdx.x] =\n",
      "                m2[(tile * TILE_WIDTH + threadIdx.y) * m2_x + c * TILE_WIDTH + x];\n",
      "\n",
      "            __syncthreads();\n",
      "\n",
      "            for (int i = 0; i < TILE_WIDTH; i++) {\n",
      "                R[c] += m1_tile[threadIdx.y][i] * m2_tile[i][threadIdx.x];\n",
      "            }\n",
      "\n",
      "            __syncthreads();\n",
      "        }\n",
      "    }\n",
      "\n",
      "    for (int c = 0; c < THREAD_COARSENING; c++) {\n",
      "        res[y * out_shape_1 + c * TILE_WIDTH + x] = R[c];\n",
      "    }\n",
      "}\n",
      "\n",
      "```\n",
      ":::\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|output: asis\n",
    "#|echo: false\n",
    "\n",
    "\n",
    "c_code = Path(cu_file).read_text()\n",
    "print(f'''\n",
    "\n",
    "[day_08_thread-coarsening.cu](https://github.com/xl0/cuda-100/blob/master/nbs/{cu_file})\n",
    "\n",
    "::: {{.code-block}}\n",
    "```\n",
    "{c_code}\n",
    "```\n",
    ":::\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_numpy import Lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compiler options for more compile-time warnings.\n",
    "warn_options=[\n",
    "    '-Xcompiler', '-Wall',\n",
    "    '-Xcompiler', '-Wextra',\n",
    "    '-Xcompiler', '-Wsign-conversion',\n",
    "    '-Xcompiler', '-Wcast-qual',\n",
    "    '-Xcompiler', '-Wunused-parameter',\n",
    "    '-Xcompiler', '-Wdouble-promotion',\n",
    "    '-Xcompiler', '-Wformat=2',\n",
    "    '-Xcompiler', '-Wfloat-equal',\n",
    "    '-Xcompiler', '-Wshadow'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matmul(ctx, kernel, m1, m2, block_size, grid_size, repeat=10, warmup=True):\n",
    "    assert len(m1.shape) == 2\n",
    "    assert len(m2.shape) == 2\n",
    "    assert m1.shape[1] == m2.shape[0]\n",
    "\n",
    "    out_shape = (m1.shape[0], m2.shape[1])\n",
    "\n",
    "    gpu_m1 = cuda.mem_alloc_like(m1)\n",
    "    gpu_m2 = cuda.mem_alloc_like(m2)\n",
    "\n",
    "    res = np.empty(out_shape, dtype=np.float32)\n",
    "\n",
    "    cuda.memcpy_htod(gpu_m1, m1)\n",
    "    cuda.memcpy_htod(gpu_m2, m2)\n",
    "    ctx.synchronize()\n",
    "\n",
    "    timing=0\n",
    "    for _ in range(repeat):\n",
    "        start = cuda.Event()\n",
    "        end = cuda.Event()\n",
    "\n",
    "        gpu_res = cuda.mem_alloc_like(res)\n",
    "\n",
    "        if warmup:\n",
    "            kernel(gpu_m1, gpu_m2, gpu_res, np.uint32(out_shape[0]), np.uint32(out_shape[1]), np.uint32(m1.shape[1]), grid=grid_size, block=block_size)\n",
    "            ctx.synchronize()\n",
    "\n",
    "        start.record()\n",
    "        kernel(gpu_m1, gpu_m2, gpu_res, np.uint32(out_shape[0]), np.uint32(out_shape[1]), np.uint32(m1.shape[1]), grid=grid_size, block=block_size)\n",
    "        end.record()\n",
    "        end.synchronize()\n",
    "\n",
    "        timing += end.time_since(start)\n",
    "    timing /= repeat\n",
    "\n",
    "    cuda.memcpy_dtoh(res, gpu_res)\n",
    "    return res, timing;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix 1 shape: (8192, 8192)\n",
      "Matrix 2 shape: (8192, 8192)\n",
      "Result shape: (8192, 8192)\n",
      "Grid size: (64, 256, 1)\n",
      "Block size: (32, 32, 1)\n",
      "Total threads: 16777216\n",
      "array[8192, 8192] f32 n=67108864 (0.2Gb) x∈[-505.431, 525.043] μ=0.007 σ=90.510\n",
      "Took 752.339ms\n"
     ]
    }
   ],
   "source": [
    "m1 = np.random.randn(8192, 8192).astype(np.float32)\n",
    "m2 = np.random.randn(8192, 8192).astype(np.float32)\n",
    "\n",
    "np_res = np.matmul(m1, m2)\n",
    "\n",
    "tile_width = 32\n",
    "coarsening = 4\n",
    "\n",
    "ctx = device.make_context()\n",
    "try:\n",
    "    mod = SourceModule(\n",
    "        Path(cu_file).read_text(),\n",
    "        options=warn_options + [\n",
    "            f\"-D TILE_WIDTH={tile_width}\",\n",
    "            f\"-D THREAD_COARSENING={coarsening}\"\n",
    "            ])\n",
    "\n",
    "    kernel = mod.get_function(\"matmul_fp32_tiled_coarse\")\n",
    "\n",
    "    out_shape = (m1.shape[0], m2.shape[1])\n",
    "\n",
    "    block_size = (tile_width, tile_width, 1)\n",
    "    grid_size = (\n",
    "        ((out_shape[1] + tile_width * coarsening - 1) // (tile_width * coarsening)),\n",
    "        ((out_shape[0] + tile_width - 1) // tile_width),\n",
    "        1\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Matrix 1 shape: {m1.shape}\")\n",
    "    print(f\"Matrix 2 shape: {m2.shape}\")\n",
    "    print(f\"Result shape: {out_shape}\")\n",
    "    print(f\"Grid size: {grid_size}\")\n",
    "    print(f\"Block size: {block_size}\")\n",
    "    print(f\"Total threads: {grid_size[0] * grid_size[1] * block_size[0] * block_size[1]}\")\n",
    "\n",
    "\n",
    "    res, timing = benchmark_matmul(ctx, kernel, m1, m2, block_size, grid_size, repeat=2, warmup=True)\n",
    "finally:\n",
    "    ctx.pop()\n",
    "    ctx.detach()\n",
    "\n",
    "print(Lo(res))\n",
    "print(f\"Took {timing:.3f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9413509666919708)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(res, np_res).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matmul 8192x8192 with tile size 32x32 and thread coarsening along x:\n",
      "coarsening = 1: 791.43ms\n",
      "coarsening = 2: 777.06ms\n",
      "coarsening = 4: 764.23ms\n",
      "coarsening = 8: 766.71ms\n"
     ]
    }
   ],
   "source": [
    "def benchmark(dim, tile_width, coarsening):\n",
    "    m1 = np.random.randn(dim, dim).astype(np.float32)\n",
    "    m2 = np.random.randn(dim, dim).astype(np.float32)\n",
    "\n",
    "    ctx = device.make_context()\n",
    "    try:\n",
    "        mod = SourceModule(\n",
    "            Path(cu_file).read_text(),\n",
    "            options=warn_options + [\n",
    "                f\"-D TILE_WIDTH={tile_width}\",\n",
    "                f\"-D THREAD_COARSENING={coarsening}\"\n",
    "                ])\n",
    "\n",
    "        kernel = mod.get_function(\"matmul_fp32_tiled_coarse\")\n",
    "\n",
    "        out_shape = (m1.shape[0], m2.shape[1])\n",
    "\n",
    "        block_size = (tile_width, tile_width, 1)\n",
    "        grid_size = (\n",
    "            ((out_shape[1] + tile_width * coarsening - 1) // (tile_width * coarsening)),\n",
    "            ((out_shape[0] + tile_width - 1) // tile_width),\n",
    "            1\n",
    "        )\n",
    "\n",
    "        # print(f\"Matrix 1 shape: {m1.shape}\")\n",
    "        # print(f\"Matrix 2 shape: {m2.shape}\")\n",
    "        # print(f\"Result shape: {out_shape}\")\n",
    "        # print(f\"Grid size: {grid_size}\")\n",
    "        # print(f\"Block size: {block_size}\")\n",
    "        # print(f\"Total threads: {grid_size[0] * grid_size[1] * block_size[0] * block_size[1]}\")\n",
    "\n",
    "\n",
    "        res, timing = benchmark_matmul(ctx, kernel, m1, m2, block_size, grid_size, repeat=2, warmup=True)\n",
    "    finally:\n",
    "        ctx.pop()\n",
    "        ctx.detach()\n",
    "\n",
    "    return res, timing\n",
    "\n",
    "\n",
    "\n",
    "print(\"Matmul 8192x8192 with tile size 32x32 and thread coarsening along x:\")\n",
    "for c in [1, 2, 4, 8]:\n",
    "    res, timing = benchmark(8192, 32, c)\n",
    "    print(f\"coarsening = {c}: {timing:.2f}ms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarsening helps, but only a small bit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
